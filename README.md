# Contextual Post Explainer

An AI agent that explains social media posts by searching for and synthesizing relevant context. Given any confusing or reference-heavy post, it returns 3-5 bullet point explanations with source citations.

## Features

- **Smart Search**: Automatically extracts key terms and searches for context
- **AI Synthesis**: Uses GPT-4 to synthesize search results into clear explanations
- **Source Citations**: Every explanation includes numbered citations to sources
- **Streaming Responses**: See explanations as they're generated in real-time
- **Caching**: Results are cached to reduce API costs and latency
- **Evaluation Harness**: 12 test cases with automated metrics

### Bonus Features

- **ğŸ–¼ï¸ Image Understanding**: Analyze images in posts using GPT-4 Vision / Claude Vision
- **ğŸ”„ Multi-Provider Comparison**: Compare GPT-4 vs Claude side-by-side
- **ğŸ§‘â€âš–ï¸ LLM-as-Judge Evaluation**: Automated quality scoring using GPT-4

## Architecture

### High-Level Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INPUT                              â”‚
â”‚  Text Post + Optional Image URL                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CACHE CHECK                                  â”‚
â”‚  Hash-based lookup (24h TTL)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Cache Miss
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              QUERY EXTRACTION                                   â”‚
â”‚  â€¢ Full post text (truncated)                                   â”‚
â”‚  â€¢ Quoted phrases                                               â”‚
â”‚  â€¢ Hashtags                                                     â”‚
â”‚  â€¢ Capitalized terms                                            â”‚
â”‚  (No LLM needed - regex-based)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WEB SEARCH                                   â”‚
â”‚  Primary: Tavily AI (optimized for AI agents)                   â”‚
â”‚  Fallback: Brave Search API (optional)                          â”‚
â”‚  Returns: 8-10 search results with snippets                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              IMAGE PROCESSING (Optional)                         â”‚
â”‚  â€¢ Download image from URL                                      â”‚
â”‚  â€¢ Encode to base64                                             â”‚
â”‚  â€¢ Prepare for vision model                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LLM SYNTHESIS                                      â”‚
â”‚  Provider: OpenAI GPT-4o (default)                              â”‚
â”‚  Alternative: Anthropic Claude (with fallback)                  â”‚
â”‚  Input: Post + Search Results + Optional Image                  â”‚
â”‚  Output: 3-5 bullet points with citations                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RESPONSE PARSING                                   â”‚
â”‚  â€¢ Extract bullet points                                        â”‚
â”‚  â€¢ Parse citations [1], [2]                                    â”‚
â”‚  â€¢ Build source list                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CACHE STORE                                  â”‚
â”‚  Save result for 24 hours                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER OUTPUT                                  â”‚
â”‚  â€¢ Bullet points                                                â”‚
â”‚  â€¢ Clickable source citations                                   â”‚
â”‚  â€¢ Streaming (SSE) or complete                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FRONTEND (React)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ PostInput    â”‚â†’ â”‚ Streaming    â”‚â†’ â”‚ Explanation     â”‚   â”‚
â”‚  â”‚ â€¢ Text       â”‚  â”‚ Display      â”‚  â”‚ Display         â”‚   â”‚
â”‚  â”‚ â€¢ Image URL  â”‚  â”‚ â€¢ SSE Client â”‚  â”‚ â€¢ Bullets       â”‚   â”‚
â”‚  â”‚ â€¢ Compare    â”‚  â”‚ â€¢ Typewriter â”‚  â”‚ â€¢ Sources       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ HTTP/SSE
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKEND (FastAPI)                          â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ API Layer (routes.py)                                  â”‚  â”‚
â”‚  â”‚ â€¢ POST /api/explain                                    â”‚  â”‚
â”‚  â”‚ â€¢ POST /api/explain/stream                             â”‚  â”‚
â”‚  â”‚ â€¢ POST /api/explain/compare                            â”‚  â”‚
â”‚  â”‚ â€¢ GET /api/providers                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                   â”‚                                            â”‚
â”‚                   â–¼                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ PostExplainer (explainer.py)                           â”‚  â”‚
â”‚  â”‚ Main orchestration service                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â”‚                                                    â”‚
â”‚          â”œâ”€â”€â†’ QueryExtractor (query_extractor.py)            â”‚
â”‚          â”‚    â€¢ Regex-based extraction                        â”‚
â”‚          â”‚    â€¢ No LLM needed                                â”‚
â”‚          â”‚                                                    â”‚
â”‚          â”œâ”€â”€â†’ SearchService (search.py)                      â”‚
â”‚          â”‚    â€¢ TavilySearchProvider                          â”‚
â”‚          â”‚    â€¢ BraveSearchProvider (fallback)               â”‚
â”‚          â”‚    â€¢ Deduplication & ranking                      â”‚
â”‚          â”‚                                                    â”‚
â”‚          â”œâ”€â”€â†’ ImageProcessor (image_processor.py)           â”‚
â”‚          â”‚    â€¢ Download & encode images                     â”‚
â”‚          â”‚    â€¢ Prepare for vision models                    â”‚
â”‚          â”‚                                                    â”‚
â”‚          â”œâ”€â”€â†’ LLMService (llm.py)                           â”‚
â”‚          â”‚    â€¢ OpenAIProvider (GPT-4o)                      â”‚
â”‚          â”‚    â€¢ AnthropicProvider (Claude)                   â”‚
â”‚          â”‚    â€¢ Model fallback logic                         â”‚
â”‚          â”‚    â€¢ Vision support                               â”‚
â”‚          â”‚                                                    â”‚
â”‚          â””â”€â”€â†’ CacheService (cache.py)                        â”‚
â”‚               â€¢ In-memory cache                              â”‚
â”‚               â€¢ SHA256-based keys                            â”‚
â”‚               â€¢ 24h TTL                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXTERNAL SERVICES                          â”‚
â”‚  â€¢ Tavily AI Search API                                       â”‚
â”‚  â€¢ OpenAI API (GPT-4o, GPT-4 Vision)                         â”‚
â”‚  â€¢ Anthropic API (Claude 3.5 Sonnet, Opus, Haiku)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Decisions

1. **Single LLM Call**: Unlike multi-step approaches, we use one LLM call for synthesis. This reduces latency (~3-5s vs ~8-12s) and cost while maintaining quality.

2. **No LLM for Query Generation**: Simple regex-based extraction works well for short social media posts. Saves an API call and reduces latency.

3. **Search Snippets Only**: We don't scrape full pages. Search snippets provide enough context and are more reliable (no 403 errors, faster).

4. **Streaming First**: The UI streams responses as they're generated for better UX. Users see results immediately.

5. **Tavily for Search**: Optimized for AI agents with high-quality snippets. Brave Search as optional fallback.

6. **Model Fallback**: Claude provider tries multiple model names automatically if one isn't available.

7. **In-Memory Cache**: Simple dict-based cache for MVP. Can be replaced with Redis for production.

8. **Provider Abstraction**: Clean abstraction allows easy addition of new LLM providers.

## Tech Stack

- **Frontend**: React 18 + TypeScript + Vite + Tailwind CSS
- **Backend**: FastAPI + Python 3.11+
- **LLM**: OpenAI GPT-4o (default), Anthropic Claude (optional)
- **Search**: Tavily AI Search API (primary), Brave Search (fallback)
- **Evaluation**: sentence-transformers for semantic similarity
- **Streaming**: Server-Sent Events (SSE)

## Setup

### Prerequisites

- Python 3.11+
- Node.js 18+
- API Keys:
  - OpenAI API key (required)
  - Tavily API key (required) - get one free at https://tavily.com
  - Anthropic API key (optional, for Claude comparison)

### 1. Clone the Repository

```bash
git clone https://github.com/charlesblakely0701-star/post-explainer.git
cd post-explainer
```

### 2. Backend Setup

```bash
# Create virtual environment
cd backend
python -m venv venv

# Activate it
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Create .env file in project root
cd ..
cp env.example .env
# Edit .env and add your API keys
```

**Required `.env` configuration:**
```env
OPENAI_API_KEY=sk-your-openai-api-key
TAVILY_API_KEY=tvly-your-tavily-api-key

# Optional
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key
BRAVE_API_KEY=your-brave-search-key
```

### 3. Frontend Setup

```bash
cd frontend
npm install
cd ..
```

### 4. Run the Application

**Terminal 1 - Backend:**
```bash
cd backend
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

**Terminal 2 - Frontend:**
```bash
cd frontend
npm run dev
```

Open http://localhost:3000 in your browser.

## API Endpoints

### POST /api/explain

Explain a social media post.

**Request:**
```json
{
  "text": "Been using the Ralph Wiggum technique all week",
  "image_url": "https://example.com/image.jpg"
}
```

**Response:**
```json
{
  "bullets": [
    "The Ralph Wiggum technique is a bash-loop method for iterating AI coding agents [1]",
    "Named after the Simpsons character for its trial-and-error style [1][2]",
    "Coined by Geoffrey Huntley in mid-2025 [2]"
  ],
  "sources": [
    {"id": 1, "title": "...", "url": "...", "snippet": "..."},
    {"id": 2, "title": "...", "url": "...", "snippet": "..."}
  ],
  "cached": false
}
```

### POST /api/explain/stream

Same as above but streams response via Server-Sent Events (SSE).

**Events:**
- `sources`: Initial sources data
- `chunk`: Text chunks as they're generated
- `done`: Completion signal

### POST /api/explain/compare

Compare explanations from multiple LLM providers (GPT-4 vs Claude).

**Request:**
```json
{
  "text": "Vibe coding is the future",
  "image_url": null
}
```

**Response:**
```json
{
  "providers": {
    "openai": {
      "bullets": ["..."],
      "sources": [...]
    },
    "anthropic": {
      "bullets": ["..."],
      "sources": [...]
    }
  },
  "available_providers": ["openai", "anthropic"]
}
```

### GET /api/providers

List available LLM providers.

**Response:**
```json
{
  "providers": ["openai", "anthropic"]
}
```

### GET /api/health

Health check endpoint.

**Response:**
```json
{
  "status": "ok",
  "version": "1.0.0"
}
```

## Evaluation Harness

The evaluation harness tests the agent against 12 diverse posts across categories (tech, culture, news, memes, finance).

### Run Evaluation

**Important:** Make sure you're in the virtual environment before running evaluation.

```bash
# Activate virtual environment first
cd backend
# On Windows:
.\venv\Scripts\Activate.ps1
# On macOS/Linux:
source venv/bin/activate

# Go back to project root
cd ..

# Run all tests
python -m evaluation.cli run

# Run a single test
python -m evaluation.cli run --case tech-01

# Run tests by category
python -m evaluation.cli run --category tech

# List available tests
python -m evaluation.cli list

# Generate report from results
python -m evaluation.cli report --format markdown
```

### Evaluation Metrics

1. **Keyword Coverage**: % of expected keywords in output
2. **Topic Coverage**: % of expected topics addressed
3. **Semantic Similarity**: Embedding similarity to reference explanation (sentence-transformers)
4. **Citation Quality**: Presence and validity of citations
5. **Format Quality**: Adherence to bullet point format

### LLM-as-Judge Evaluation

Use GPT-4 to evaluate explanation quality:

```bash
# First run regular evaluation
python -m evaluation.cli run

# Then run LLM judge on results
python evaluation/llm_judge.py evaluation/results/results_YYYYMMDD_HHMMSS.json
```

**LLM Judge Metrics:**
- Accuracy (1-5): Factual correctness
- Relevance (1-5): Addresses the post's context
- Completeness (1-5): Covers main points
- Clarity (1-5): Easy to understand
- Citation Quality (1-5): Proper source attribution

### Test Cases

| ID | Category | Difficulty | Post Preview |
|----|----------|------------|--------------|
| tech-01 | tech | medium | Ralph Wiggum technique... |
| tech-02 | tech | easy | Enshittification of the internet... |
| tech-03 | tech | easy | Vibe coding is the future... |
| tech-04 | tech | easy | Just discovered htmx... |
| tech-05 | tech | easy | Local LLMs are getting scary good... |
| culture-01 | culture | medium | Lindy effect... |
| culture-02 | culture | easy | Touch grass energy... |
| news-01 | news | easy | OpenAI announced GPT-5... |
| news-02 | news | medium | Dead Internet Theory... |
| meme-01 | meme | easy | 10x engineer... |
| meme-02 | meme | easy | Skill issue tbh... |
| finance-01 | finance | medium | Diamond hands... WAGMI... |

## Project Structure

```
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                    # FastAPI entry point
â”‚   â”œâ”€â”€ config.py                  # Configuration & env loading
â”‚   â”œâ”€â”€ prompts.py                 # LLM prompt templates
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes.py              # API endpoints
â”‚   â”‚   â””â”€â”€ errors.py              # Custom exception handlers
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ explainer.py           # Main orchestration
â”‚   â”‚   â”œâ”€â”€ search.py              # Search providers (Tavily, Brave)
â”‚   â”‚   â”œâ”€â”€ llm.py                  # LLM providers (OpenAI, Anthropic)
â”‚   â”‚   â”œâ”€â”€ query_extractor.py     # Regex-based query extraction
â”‚   â”‚   â”œâ”€â”€ cache.py               # In-memory caching
â”‚   â”‚   â””â”€â”€ image_processor.py     # Image download & encoding
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py             # Pydantic request/response models
â”‚   â”‚
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx                # Main React component
â”‚   â”‚   â”œâ”€â”€ api/client.ts          # API client (fetch, SSE)
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ PostInput.tsx      # Input form with image URL
â”‚   â”‚   â”‚   â”œâ”€â”€ ExplanationDisplay.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ComparisonDisplay.tsx  # Side-by-side provider comparison
â”‚   â”‚   â”‚   â”œâ”€â”€ ExamplePosts.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ErrorDisplay.tsx
â”‚   â”‚   â”‚   â””â”€â”€ LoadingSkeleton.tsx
â”‚   â”‚   â””â”€â”€ data/examples.ts       # Example posts
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ cli.py                     # CLI entry point
â”‚   â”œâ”€â”€ runner.py                  # Async test runner
â”‚   â”œâ”€â”€ metrics.py                 # Evaluation metrics (5 types)
â”‚   â”œâ”€â”€ llm_judge.py               # LLM-as-judge evaluation
â”‚   â”œâ”€â”€ test_cases.json            # 12 test cases
â”‚   â””â”€â”€ results/                   # Output directory
â”‚
â”œâ”€â”€ .env.example                   # Environment template
â”œâ”€â”€ PLAN.md                        # Implementation plan
â””â”€â”€ README.md                      # This file
```

## Performance Characteristics

- **Average Response Time**: 3-5 seconds (non-streaming)
- **Streaming Start**: ~1-2 seconds (first chunk)
- **Cache Hit Rate**: ~30-50% for repeated queries
- **Search Results**: 8-10 per query
- **LLM Tokens**: ~500-800 per explanation

## Future Improvements

- [x] **Image Understanding**: Use GPT-4 Vision for posts with images âœ…
- [x] **Multi-Provider Comparison**: Compare explanations from different LLMs âœ…
- [x] **LLM-as-Judge**: Add GPT-4 based evaluation for quality scoring âœ…
- [ ] **Redis Caching**: Replace in-memory cache for production
- [ ] **Rate Limiting**: Add request rate limiting
- [ ] **User Feedback**: Allow users to rate explanations
- [ ] **Batch Processing**: Process multiple posts at once
- [ ] **Custom Prompts**: Allow users to customize explanation style


